{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "516275fd-2bbc-41c3-b4af-33f4f49be2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: sensor object went out of the scope but the sensor is still alive in the simulation: Actor 34 (sensor.lidar.ray_cast) \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import cv2 #to work with images from cameras\n",
    "import carla #the sim library itself\n",
    "import time # to set a delay after each photo\n",
    "import numpy as np #in this example to change image representation - re-shaping\n",
    "import math\n",
    "import sys\n",
    "client = carla.Client('localhost', 2000)\n",
    "world = client.get_world()\n",
    "sys.path.append('/home/youmad55/Downloads/CARLA_0.9.15/PythonAPI/carla/')\n",
    "from agents.navigation.global_route_planner import GlobalRoutePlanner\n",
    "grp = GlobalRoutePlanner(world.get_map(), 1)\n",
    "\n",
    "CAMERA_POS_Z = 2\n",
    "CAMERA_POS_X = 2 \n",
    "\n",
    "#adding params to display text to image\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# org - defining lines to display telemetry values on the screen\n",
    "org = (30, 30) # this line will be used to show current speed\n",
    "org2 = (30, 50) # this line will be used for future steering angle\n",
    "org3 = (30, 70) # and another line for future telemetry outputs\n",
    "org4 = (30, 90) # and another line for future telemetry outputs\n",
    "org3 = (30, 110) # and another line for future telemetry outputs\n",
    "fontScale = 0.5\n",
    "# white color\n",
    "color = (255, 255, 255)\n",
    "# Line thickness of 2 px\n",
    "thickness = 1\n",
    "\n",
    "# define speed contstants\n",
    "PREFERRED_SPEED = 30 # what it says\n",
    "PREFERRED_SPEED2 = 15 # what it says\n",
    "SPEED_THRESHOLD = 2 #defines when we get close to desired speed so we drop the\n",
    "\n",
    "# Max steering angle\n",
    "MAX_STEER_DEGREES = 50\n",
    "\n",
    "\n",
    "def collision_check(point, obstacles):\n",
    "    collision_free = True\n",
    "    for k in range(len(obstacles)):\n",
    "        collision_dist=math.sqrt((point[0]-obstacles[k][0])**2+(point[1]-obstacles[k][1])**2)\n",
    "        if collision_dist<15:\n",
    "            collision_free=False\n",
    "            break\n",
    "\n",
    "    return collision_free\n",
    "def get_short_dist(point,obstacles):\n",
    "    min_dist=float('inf')\n",
    "    for k in range(len(obstacles)):\n",
    "        collision_dist=math.sqrt((point[0]-obstacles[k][0])**2+(point[1]-obstacles[k][1])**2)\n",
    "        if collision_dist<min_dist:\n",
    "            min_dist=collision_dist\n",
    "\n",
    "    return min_dist\n",
    "\n",
    "def semantic_lidar_data(point_cloud_data,car,l):\n",
    "    l.clear()\n",
    "    c=math.cos(car.get_transform().rotation.yaw*math.pi/180)\n",
    "    s=math.sin(car.get_transform().rotation.yaw*math.pi/180)\n",
    "    for detection in point_cloud_data:\n",
    "        #l.append([round(detection.point.x+car.get_location().x,1),round(detection.point.y+car.get_location().y,1)])\n",
    "        #l.append([detection.point.x+car.get_location().x,detection.point.y+car.get_location().y])\n",
    "        l.append([round((detection.point.x *c +detection.point.y*-s)+car.get_location().x,1),round((detection.point.x*s+detection.point.y*c)+car.get_location().y,1)])\n",
    "\n",
    "def generate_lidar_blueprint(blueprint_library):\n",
    "    lidar_blueprint = blueprint_library.find('sensor.lidar.ray_cast')\n",
    "    lidar_blueprint.set_attribute('channels', str(32))\n",
    "    lidar_blueprint.set_attribute(\"points_per_second\",str(56000))\n",
    "    lidar_blueprint.set_attribute(\"rotation_frequency\",str(1000))\n",
    "    lidar_blueprint.set_attribute(\"range\",str(50))\n",
    "    lidar_blueprint.set_attribute(\"upper_fov\",str(1))\n",
    "    lidar_blueprint.set_attribute(\"lower_fov\",str(-1))\n",
    "#    lidar_blueprint.set_attribute(\"horizontal_fov\",str(360))\n",
    "    return lidar_blueprint\n",
    "\n",
    "\n",
    "# maintain speed function\n",
    "def maintain_speed(s):\n",
    "    ''' \n",
    "    this is a very simple function to maintan desired speed\n",
    "    s arg is actual current speed\n",
    "    '''\n",
    "    if s >= PREFERRED_SPEED:\n",
    "        return 0\n",
    "    elif s < PREFERRED_SPEED - SPEED_THRESHOLD:\n",
    "        return 0.9 # think of it as % of \"full gas\"\n",
    "    else:\n",
    "        return 0.4 # tweak this if the car is way over or under preferred speed \n",
    "def maintain_speed2(s):\n",
    "    ''' \n",
    "    this is a very simple function to maintan desired speed\n",
    "    s arg is actual current speed\n",
    "    '''\n",
    "    if s >= PREFERRED_SPEED2:\n",
    "        return 0\n",
    "    elif s < PREFERRED_SPEED2 - SPEED_THRESHOLD:\n",
    "        return 0.9 # think of it as % of \"full gas\"\n",
    "    else:\n",
    "        return 0.4 # tweak this if the car is way over or under preferred speed \n",
    "\n",
    "wa=world.get_map().get_waypoint(carla.Location(x=-60,y=137,z=0.5))\n",
    "start_point=wa.transform\n",
    "start_point.location.z=0.5\n",
    "midle_point=wa.transform\n",
    "midle_point.location.z=0.5\n",
    "midle_point.location.x=-30\n",
    "\n",
    "vehicle_bp = world.get_blueprint_library().filter('*model3*')\n",
    "vehicle = world.try_spawn_actor(vehicle_bp[0], start_point)\n",
    "vehicle2 = world.try_spawn_actor(vehicle_bp[0], midle_point)\n",
    "\n",
    "get_blueprint_of_world = world.get_blueprint_library()\n",
    "lidar_sensor = generate_lidar_blueprint(get_blueprint_of_world)\n",
    "sensor_lidar_spawn_point = carla.Transform(carla.Location(x=0, y=0, z=1.2),\n",
    "                                           carla.Rotation(pitch=0.000000, yaw=0.0, roll=0.000000))\n",
    "sensor = world.spawn_actor(lidar_sensor, sensor_lidar_spawn_point, attach_to=vehicle)\n",
    "wa=world.get_map().get_waypoint(carla.Location(x=60,y=137,z=0.5))\n",
    "end_point=wa.transform.location\n",
    "\n",
    "route = grp.trace_route(start_point.location, end_point)\n",
    "\n",
    "curIndx=1\n",
    "\n",
    "curr_wp = 1\n",
    "\n",
    "ladarData=[]\n",
    "sensor.listen(lambda point_cloud_data: semantic_lidar_data(point_cloud_data,vehicle,ladarData))\n",
    "\n",
    "ladarData1=ladarData.copy()\n",
    "\n",
    "camera_bp = world.get_blueprint_library().find('sensor.camera.depth')\n",
    "camera_bp.set_attribute('image_size_x', '640') # this ratio works in CARLA 9.14 on Windows\n",
    "camera_bp.set_attribute('image_size_y', '360')\n",
    "\n",
    "camera_init_trans = carla.Transform(carla.Location(z=CAMERA_POS_Z,x=CAMERA_POS_X))\n",
    "#this creates the camera in the sim\n",
    "camera = world.spawn_actor(camera_bp,camera_init_trans,attach_to=vehicle)\n",
    "\n",
    "def camera_callback(image,data_dict):\n",
    "    data_dict['image'] = np.reshape(np.copy(image.raw_data),(image.height,image.width,4))\n",
    "\n",
    "image_w = camera_bp.get_attribute('image_size_x').as_int()\n",
    "image_h = camera_bp.get_attribute('image_size_y').as_int()\n",
    "\n",
    "camera_data = {'image': np.zeros((image_h,image_w,4))}\n",
    "# this actually opens a live stream from the camera\n",
    "camera.listen(lambda image: camera_callback(image,camera_data))\n",
    "\n",
    "cv2.namedWindow('depth Camera',cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('depth Camera',camera_data['image'])\n",
    "flag=0\n",
    "BRAKE=0\n",
    "curr_wp = 5 #we will be tracking waypoints in the route and switch to next one wen we get close to current one\n",
    "while curr_wp<len(route)-11:\n",
    "    \n",
    "    world.tick()\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        quit = True\n",
    "        vehicle.apply_control(carla.VehicleControl(throttle=0,steer=0,brake=1))\n",
    "        break\n",
    "    image = camera_data['image']\n",
    "    new_image=depth_to_array(image)\n",
    "    ladarData1=ladarData.copy()\n",
    "\n",
    "    xf=len(ladarData1)\n",
    "    for i in range(xf):\n",
    "        if -ladarData1[xf-1-i][0]+vehicle.get_location().x>3 or abs(ladarData1[xf-1-i][1]-vehicle.get_location().y)>3:\n",
    "            ladarData1.pop(xf-1-i)\n",
    "\n",
    "#    for POINT in ladarData1:\n",
    "#        world.debug.draw_string(carla.Location(x=POINT[0],y=POINT[1]), '^', draw_shadow=False,\n",
    "#            color=carla.Color(r=0, g=0, b=255), life_time=1.0,\n",
    "#            persistent_lines=True) \n",
    "\n",
    "    while curr_wp<len(route) and vehicle.get_transform().location.distance(route[curr_wp][0].transform.location)<5:\n",
    "        curr_wp +=1 #move to next wp if we are too close\n",
    "\n",
    "    v = vehicle.get_velocity()\n",
    "    v2 = vehicle2.get_velocity()\n",
    "    speed = round(3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2),0)\n",
    "    speed2 = round(3.6 * math.sqrt(v2.x**2 + v2.y**2 + v2.z**2),0)\n",
    "    #image = cv2.putText(image, 'Speed: '+str(int(speed)), org2, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "    estimated_throttle = maintain_speed(speed)\n",
    "    estimated_throttle2 = maintain_speed2(speed2)\n",
    "    steer_input = 0\n",
    "    steer_input = steer_input/75\n",
    "    vehicle2.apply_control(carla.VehicleControl(throttle=estimated_throttle2, steer=0))\n",
    "    if get_short_dist([vehicle.get_location().x,vehicle.get_location().y],ladarData1)<20:\n",
    "        if flag==1:\n",
    "            start_dist=end_dist\n",
    "            strat_time=end_time\n",
    "            end_dist=get_short_dist([vehicle.get_location().x,vehicle.get_location().y],ladarData1)\n",
    "            end_time=time.time()\n",
    "\n",
    "            PREFERRED_SPEED=((end_dist-start_dist)/(end_time-strat_time))+speed\n",
    "            if PREFERRED_SPEED+5<speed:\n",
    "                estimated_throttle=0\n",
    "                BRAKE=1\n",
    "            else:\n",
    "                BRAKE=0\n",
    "        end_dist=get_short_dist([vehicle.get_location().x,vehicle.get_location().y],ladarData1)\n",
    "        end_time=time.time()\n",
    "        flag=1\n",
    "    else:\n",
    "        PREFERRED_SPEED=50\n",
    "    vehicle.apply_control(carla.VehicleControl(throttle=estimated_throttle, steer=steer_input,brake=BRAKE))\n",
    "    cv2.imshow('depth Camera',new_image)\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "camera.stop()\n",
    "\n",
    "for sensor in world.get_actors().filter('*sensor*'):\n",
    "    sensor.destroy()\n",
    "for actor in world.get_actors().filter('*vehicle*'):\n",
    "    actor.destroy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd6379ad-3870-449a-97ab-3e4751711aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()\n",
    "camera.stop()\n",
    "\n",
    "for sensor in world.get_actors().filter('*sensor*'):\n",
    "    sensor.destroy()\n",
    "for actor in world.get_actors().filter('*vehicle*'):\n",
    "    actor.destroy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bb420a8-a3ee-4d16-a093-a96d54e14c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2017 Computer Vision Center (CVC) at the Universitat Autonoma de\n",
    "# Barcelona (UAB).\n",
    "#\n",
    "# This work is licensed under the terms of the MIT license.\n",
    "# For a copy, see <https://opensource.org/licenses/MIT>.\n",
    "\n",
    "\"\"\"\n",
    "Handy conversions for CARLA images.\n",
    "\n",
    "The functions here are provided for real-time display, if you want to save the\n",
    "converted images, save the images from Python without conversion and convert\n",
    "them afterwards with the C++ implementation at \"Util/ImageConverter\" as it\n",
    "provides considerably better performance.\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "\n",
    "try:\n",
    "    import numpy\n",
    "    from numpy.matlib import repmat\n",
    "except ImportError:\n",
    "    raise RuntimeError('cannot import numpy, make sure numpy package is installed')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def to_bgra_array(image):\n",
    "    \"\"\"Convert a CARLA raw image to a BGRA numpy array.\"\"\"\n",
    "    if not isinstance(image, sensor.Image):\n",
    "        raise ValueError(\"Argument must be a carla.sensor.Image\")\n",
    "    array = numpy.frombuffer(image.raw_data, dtype=numpy.dtype(\"uint8\"))\n",
    "    array = numpy.reshape(array, (image.height, image.width, 4))\n",
    "    return array\n",
    "\n",
    "\n",
    "def to_rgb_array(image):\n",
    "    \"\"\"Convert a CARLA raw image to a RGB numpy array.\"\"\"\n",
    "    array = to_bgra_array(image)\n",
    "    # Convert BGRA to RGB.\n",
    "    array = array[:, :, :3]\n",
    "    array = array[:, :, ::-1]\n",
    "    return array\n",
    "\n",
    "\n",
    "def labels_to_array(image):\n",
    "    \"\"\"\n",
    "    Convert an image containing CARLA semantic segmentation labels to a 2D array\n",
    "    containing the label of each pixel.\n",
    "    \"\"\"\n",
    "    return to_bgra_array(image)[:, :, 2]\n",
    "\n",
    "\n",
    "def labels_to_cityscapes_palette(image):\n",
    "    \"\"\"\n",
    "    Convert an image containing CARLA semantic segmentation labels to\n",
    "    Cityscapes palette.\n",
    "    \"\"\"\n",
    "    classes = {\n",
    "        0: [0, 0, 0],         # None\n",
    "        1: [70, 70, 70],      # Buildings\n",
    "        2: [190, 153, 153],   # Fences\n",
    "        3: [72, 0, 90],       # Other\n",
    "        4: [220, 20, 60],     # Pedestrians\n",
    "        5: [153, 153, 153],   # Poles\n",
    "        6: [157, 234, 50],    # RoadLines\n",
    "        7: [128, 64, 128],    # Roads\n",
    "        8: [244, 35, 232],    # Sidewalks\n",
    "        9: [107, 142, 35],    # Vegetation\n",
    "        10: [0, 0, 255],      # Vehicles\n",
    "        11: [102, 102, 156],  # Walls\n",
    "        12: [220, 220, 0]     # TrafficSigns\n",
    "    }\n",
    "    array = labels_to_array(image)\n",
    "    result = numpy.zeros((array.shape[0], array.shape[1], 3))\n",
    "    for key, value in classes.items():\n",
    "        result[numpy.where(array == key)] = value\n",
    "    return result\n",
    "\n",
    "\n",
    "def depth_to_array(image):\n",
    "    \"\"\"\n",
    "    Convert an image containing CARLA encoded depth-map to a 2D array containing\n",
    "    the depth value of each pixel normalized between [0.0, 1.0].\n",
    "    \"\"\"\n",
    "    array = image\n",
    "    array = array.astype(numpy.float32)\n",
    "    # Apply (R + G * 256 + B * 256 * 256) / (256 * 256 * 256 - 1).\n",
    "    normalized_depth = numpy.dot(array[:, :, :3], [65536.0, 256.0, 1.0])\n",
    "    normalized_depth /= 1000000.0  # (256.0 * 256.0 * 256.0 - 1.0)\n",
    "    return normalized_depth\n",
    "\n",
    "\n",
    "def depth_to_logarithmic_grayscale(image):\n",
    "    \"\"\"\n",
    "    Convert an image containing CARLA encoded depth-map to a logarithmic\n",
    "    grayscale image array.\n",
    "    \"max_depth\" is used to omit the points that are far enough.\n",
    "    \"\"\"\n",
    "    normalized_depth = depth_to_array(image)\n",
    "    # Convert to logarithmic depth.\n",
    "    logdepth = numpy.ones(normalized_depth.shape) + \\\n",
    "        (numpy.log(normalized_depth) / 5.70378)\n",
    "    logdepth = numpy.clip(logdepth, 0.0, 1.0)\n",
    "    logdepth *= 255.0\n",
    "    # Expand to three colors.\n",
    "    return numpy.repeat(logdepth[:, :, numpy.newaxis], 3, axis=2)\n",
    "\n",
    "\n",
    "def depth_to_local_point_cloud(image,vehicle, color=None, max_depth=0.9):\n",
    "    \"\"\"\n",
    "    Convert an image containing CARLA encoded depth-map to a 2D array containing\n",
    "    the 3D position (relative to the camera) of each pixel and its corresponding\n",
    "    RGB color of an array.\n",
    "    \"max_depth\" is used to omit the points that are far enough.\n",
    "    \"\"\"\n",
    "    far = 59.6  # max depth in meters.\n",
    "    normalized_depth = depth_to_array(image)\n",
    "\n",
    "    # (Intrinsic) K Matrix\n",
    "    k = numpy.identity(3)\n",
    "    k[0, 2] = 512 / 2.0\n",
    "    k[1, 2] = 424 / 2.0\n",
    "    k[0, 0] = k[1, 1] = 512 / \\\n",
    "        (2.0 * math.tan(70 * math.pi / 360.0))\n",
    "\n",
    "    # 2d pixel coordinates\n",
    "    pixel_length = 512 * 424\n",
    "    u_coord = repmat(numpy.r_[512-1:-1:-1],\n",
    "                     424, 1).reshape(pixel_length)\n",
    "    v_coord = repmat(numpy.c_[424-1:-1:-1],\n",
    "                     1, 512).reshape(pixel_length)\n",
    "    if color is not None:\n",
    "        color = color.reshape(pixel_length, 3)\n",
    "    normalized_depth = numpy.reshape(normalized_depth, pixel_length)\n",
    "\n",
    "    # Search for pixels where the depth is greater than max_depth to\n",
    "    # delete them\n",
    "    max_depth_indexes = numpy.where(normalized_depth > max_depth)\n",
    "    normalized_depth = numpy.delete(normalized_depth, max_depth_indexes)\n",
    "    u_coord = numpy.delete(u_coord, max_depth_indexes)\n",
    "    v_coord = numpy.delete(v_coord, max_depth_indexes)\n",
    "    if color is not None:\n",
    "        color = numpy.delete(color, max_depth_indexes, axis=0)\n",
    "\n",
    "    # pd2 = [u,v,1]\n",
    "    p2d = numpy.array([u_coord, v_coord, numpy.ones_like(u_coord)])\n",
    "\n",
    "    # P = [Y,Z,X]\n",
    "    p3d = numpy.dot(numpy.linalg.inv(k), p2d)\n",
    "    p3d *= normalized_depth * far\n",
    "    p3d[0]*=-1\n",
    "    p3d[2]+=vehicle.get_location().x+2\n",
    "    p3d[0]+=vehicle.get_location().y\n",
    "    p3d[1]+=vehicle.get_location().z+2\n",
    "    # Formating the output to:\n",
    "    # [[X1,Y1,Z1,R1,G1,B1],[X2,Y2,Z2,R2,G2,B2], ... [Xn,Yn,Zn,Rn,Gn,Bn]]\n",
    "    if color is not None:\n",
    "        # numpy.concatenate((numpy.transpose(p3d), color), axis=1)\n",
    "        return sensor.PointCloud(\n",
    "            image.frame_number,\n",
    "            numpy.transpose(p3d),\n",
    "            color_array=color)\n",
    "    # [[X1,Y1,Z1],[X2,Y2,Z2], ... [Xn,Yn,Zn]]\n",
    "    return numpy.transpose(p3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39356b78-2fde-44b5-b92d-92cfddf14cb9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2 #to work with images from cameras\n",
    "import carla #the sim library itself\n",
    "import time # to set a delay after each photo\n",
    "import numpy as np #in this example to change image representation - re-shaping\n",
    "import math\n",
    "import sys\n",
    "#import open3d as o3d\n",
    "\n",
    "client = carla.Client('localhost', 2000)\n",
    "world = client.get_world()\n",
    "sys.path.append('/home/youmad55/Downloads/CARLA_0.9.15/PythonAPI/carla/')\n",
    "from agents.navigation.global_route_planner import GlobalRoutePlanner\n",
    "grp = GlobalRoutePlanner(world.get_map(), 1)\n",
    "\n",
    "CAMERA_POS_Z = 2\n",
    "CAMERA_POS_X = 2 \n",
    "\n",
    "# Depth camera parameters:\n",
    "FX_DEPTH = 5.8262448167737955e+02\n",
    "FY_DEPTH = 5.8269103270988637e+02\n",
    "CX_DEPTH = 3.1304475870804731e+02\n",
    "CY_DEPTH = 2.3844389626620386e+02\n",
    "\n",
    "#adding params to display text to image\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# org - defining lines to display telemetry values on the screen\n",
    "org = (30, 30) # this line will be used to show current speed\n",
    "org2 = (30, 50) # this line will be used for future steering angle\n",
    "org3 = (30, 70) # and another line for future telemetry outputs\n",
    "org4 = (30, 90) # and another line for future telemetry outputs\n",
    "org3 = (30, 110) # and another line for future telemetry outputs\n",
    "fontScale = 0.5\n",
    "# white color\n",
    "color = (255, 255, 255)\n",
    "# Line thickness of 2 px\n",
    "thickness = 1\n",
    "\n",
    "# define speed contstants\n",
    "PREFERRED_SPEED = 30 # what it says\n",
    "PREFERRED_SPEED2 = 15 # what it says\n",
    "SPEED_THRESHOLD = 2 #defines when we get close to desired speed so we drop the\n",
    "\n",
    "# Max steering angle\n",
    "MAX_STEER_DEGREES = 50\n",
    "\n",
    "\n",
    "def collision_check(point, obstacles):\n",
    "    collision_free = True\n",
    "    for k in range(len(obstacles)):\n",
    "        collision_dist=math.sqrt((point[0]-obstacles[k][0])**2+(point[1]-obstacles[k][1])**2)\n",
    "        if collision_dist<15:\n",
    "            collision_free=False\n",
    "            break\n",
    "\n",
    "    return collision_free\n",
    "def get_short_dist(point,obstacles):\n",
    "    min_dist=float('inf')\n",
    "    for k in range(len(obstacles)):\n",
    "        collision_dist=math.sqrt((point[0]-obstacles[k][0])**2+(point[1]-obstacles[k][1])**2)\n",
    "        if collision_dist<min_dist:\n",
    "            min_dist=collision_dist\n",
    "\n",
    "    return min_dist\n",
    "\n",
    "def semantic_lidar_data(point_cloud_data,car,l):\n",
    "    l.clear()\n",
    "    c=math.cos(car.get_transform().rotation.yaw*math.pi/180)\n",
    "    s=math.sin(car.get_transform().rotation.yaw*math.pi/180)\n",
    "    for detection in point_cloud_data:\n",
    "        #l.append([round(detection.point.x+car.get_location().x,1),round(detection.point.y+car.get_location().y,1)])\n",
    "        #l.append([detection.point.x+car.get_location().x,detection.point.y+car.get_location().y])\n",
    "        l.append([round((detection.point.x *c +detection.point.y*-s)+car.get_location().x,1),round((detection.point.x*s+detection.point.y*c)+car.get_location().y,1)])\n",
    "\n",
    "def generate_lidar_blueprint(blueprint_library):\n",
    "    lidar_blueprint = blueprint_library.find('sensor.lidar.ray_cast')\n",
    "    lidar_blueprint.set_attribute('channels', str(32))\n",
    "    lidar_blueprint.set_attribute(\"points_per_second\",str(56000))\n",
    "    lidar_blueprint.set_attribute(\"rotation_frequency\",str(1000))\n",
    "    lidar_blueprint.set_attribute(\"range\",str(50))\n",
    "    lidar_blueprint.set_attribute(\"upper_fov\",str(1))\n",
    "    lidar_blueprint.set_attribute(\"lower_fov\",str(-1))\n",
    "#    lidar_blueprint.set_attribute(\"horizontal_fov\",str(360))\n",
    "    return lidar_blueprint\n",
    "\n",
    "\n",
    "# maintain speed function\n",
    "def maintain_speed(s):\n",
    "    ''' \n",
    "    this is a very simple function to maintan desired speed\n",
    "    s arg is actual current speed\n",
    "    '''\n",
    "    if s >= PREFERRED_SPEED:\n",
    "        return 0\n",
    "    elif s < PREFERRED_SPEED - SPEED_THRESHOLD:\n",
    "        return 0.9 # think of it as % of \"full gas\"\n",
    "    else:\n",
    "        return 0.4 # tweak this if the car is way over or under preferred speed \n",
    "def maintain_speed2(s):\n",
    "    ''' \n",
    "    this is a very simple function to maintan desired speed\n",
    "    s arg is actual current speed\n",
    "    '''\n",
    "    if s >= PREFERRED_SPEED2:\n",
    "        return 0\n",
    "    elif s < PREFERRED_SPEED2 - SPEED_THRESHOLD:\n",
    "        return 0.9 # think of it as % of \"full gas\"\n",
    "    else:\n",
    "        return 0.4 # tweak this if the car is way over or under preferred speed \n",
    "\n",
    "wa=world.get_map().get_waypoint(carla.Location(x=-60,y=137,z=0.5))\n",
    "start_point=wa.transform\n",
    "start_point.location.z=0.5\n",
    "midle_point=wa.transform\n",
    "midle_point.location.z=0.5\n",
    "midle_point.location.x=-50\n",
    "\n",
    "vehicle_bp = world.get_blueprint_library().filter('*model3*')\n",
    "vehicle = world.try_spawn_actor(vehicle_bp[0], start_point)\n",
    "vehicle2 = world.try_spawn_actor(vehicle_bp[0], midle_point)\n",
    "\n",
    "\n",
    "camera_bp = world.get_blueprint_library().find('sensor.camera.depth')\n",
    "camera_bp.set_attribute('image_size_x', '512') # this ratio works in CARLA 9.14 on Windows\n",
    "camera_bp.set_attribute('image_size_y', '424')\n",
    "camera_bp.set_attribute('fov', '70')\n",
    "\n",
    "camera_init_trans = carla.Transform(carla.Location(z=CAMERA_POS_Z,x=CAMERA_POS_X))\n",
    "#this creates the camera in the sim\n",
    "camera = world.spawn_actor(camera_bp,camera_init_trans,attach_to=vehicle)\n",
    "\n",
    "def camera_callback(image,data_dict):\n",
    "    data_dict['image'] = np.reshape(np.copy(image.raw_data),(image.height,image.width,4))\n",
    "\n",
    "image_w = camera_bp.get_attribute('image_size_x').as_int()\n",
    "image_h = camera_bp.get_attribute('image_size_y').as_int()\n",
    "\n",
    "camera_data = {'image': np.zeros((image_h,image_w,4))}\n",
    "# this actually opens a live stream from the camera\n",
    "camera.listen(lambda image: camera_callback(image,camera_data))\n",
    "\n",
    "cv2.namedWindow('depth Camera',cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('depth Camera',camera_data['image'])\n",
    "while True:\n",
    "    \n",
    "    world.tick()\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        quit = True\n",
    "        vehicle.apply_control(carla.VehicleControl(throttle=0,steer=0,brake=1))\n",
    "        break\n",
    "    image = camera_data['image']\n",
    "    new_image=depth_to_array(image)\n",
    "    POINTS=depth_to_local_point_cloud(image,vehicle,max_depth=0.2)\n",
    "#    pcd = []\n",
    "#    height, width = new_image.shape\n",
    "#    for i in range(height):\n",
    "#       for j in range(width):\n",
    "#           z = new_image[i][j]\n",
    "#           x = (j - CX_DEPTH) * z / FX_DEPTH\n",
    "#           y = (i - CY_DEPTH) * z / FY_DEPTH\n",
    "#           pcd.append([x, y, z])\n",
    "    \n",
    "#    for POINT in pcd:\n",
    "#        world.debug.draw_string(carla.Location(x=POINT[0],y=POINT[1],z=POINT[2]), '^', draw_shadow=False,\n",
    "#            color=carla.Color(r=0, g=0, b=255), life_time=0.00001,\n",
    "#            persistent_lines=True) \n",
    "\n",
    "#    pcd_o3d = o3d.geometry.PointCloud()  # create point cloud object\n",
    "#    pcd_o3d.points = o3d.utility.Vector3dVector(pcd)  # set pcd_np as the point cloud points\n",
    "    # Visualize:\n",
    "#    o3d.visualization.draw_geometries([pcd_o3d])\n",
    "\n",
    "    new_image2=new_image.copy()\n",
    "    for x in range(len(new_image2)):\n",
    "        for y in range(len(new_image2[x])):\n",
    "            if new_image2[x][y]<0.2 and x<280:\n",
    "                new_image2[x][y]=16\n",
    "\n",
    "    min_val=float('inf')\n",
    "    for x in range(len(new_image)):\n",
    "        for y in range(len(new_image[x])):\n",
    "            if new_image[x][y]<min_val and x<280:\n",
    "                min_val=new_image[x][y]\n",
    "\n",
    "    #image = cv2.putText(image, 'Speed: '+str(int(speed)), org2, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "    cv2.imshow('depth Camera',new_image2)\n",
    "\n",
    "#print(POINTS)\n",
    "for POINT in POINTS:\n",
    "    if POINT[1]>0.5 and abs(POINT[0]-vehicle.get_location().y)<2:\n",
    "        world.debug.draw_string(carla.Location(x=POINT[2],y=POINT[0],z=POINT[1]), '^', draw_shadow=False,\n",
    "            color=carla.Color(r=0, g=0, b=255), life_time=30,\n",
    "            persistent_lines=True) \n",
    "\n",
    "#for POINT in POINTS:\n",
    "#    print(POINT)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "camera.stop()\n",
    "\n",
    "for sensor in world.get_actors().filter('*sensor*'):\n",
    "    sensor.destroy()\n",
    "for actor in world.get_actors().filter('*vehicle*'):\n",
    "    actor.destroy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "151f0900-9013-4c18-a6dd-3d6533348d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import carla #the sim library itself\n",
    "client = carla.Client('localhost', 2000)\n",
    "world = client.get_world()\n",
    "for sensor in world.get_actors().filter('*sensor*'):\n",
    "    sensor.destroy()\n",
    "for actor in world.get_actors().filter('*vehicle*'):\n",
    "    actor.destroy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f1c527b-f696-4c09-9cb3-1bd97a115955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2 #to work with images from cameras\n",
    "import carla #the sim library itself\n",
    "import time # to set a delay after each photo\n",
    "import numpy as np #in this example to change image representation - re-shaping\n",
    "import math\n",
    "import sys\n",
    "import open3d as o3d\n",
    "\n",
    "client = carla.Client('localhost', 2000)\n",
    "world = client.get_world()\n",
    "sys.path.append('/home/youmad55/Downloads/CARLA_0.9.15/PythonAPI/carla/')\n",
    "from agents.navigation.global_route_planner import GlobalRoutePlanner\n",
    "grp = GlobalRoutePlanner(world.get_map(), 1)\n",
    "\n",
    "CAMERA_POS_Z = 2\n",
    "CAMERA_POS_X = 2 \n",
    "\n",
    "def add_open3d_axis(vis):\n",
    "    \"\"\"Add a small 3D axis on Open3D Visualizer\"\"\"\n",
    "    axis = o3d.geometry.LineSet()\n",
    "    axis.points = o3d.utility.Vector3dVector(np.array([\n",
    "        [0.0, 0.0, 0.0],\n",
    "        [1.0, 0.0, 0.0],\n",
    "        [0.0, 1.0, 0.0],\n",
    "        [0.0, 0.0, 1.0]]))\n",
    "    axis.lines = o3d.utility.Vector2iVector(np.array([\n",
    "        [0, 1],\n",
    "        [0, 2],\n",
    "        [0, 3]]))\n",
    "    axis.colors = o3d.utility.Vector3dVector(np.array([\n",
    "        [1.0, 0.0, 0.0],\n",
    "        [0.0, 1.0, 0.0],\n",
    "        [0.0, 0.0, 1.0]]))\n",
    "    vis.add_geometry(axis)\n",
    "\n",
    "\n",
    "#adding params to display text to image\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "# org - defining lines to display telemetry values on the screen\n",
    "org = (30, 30) # this line will be used to show current speed\n",
    "org2 = (30, 50) # this line will be used for future steering angle\n",
    "org3 = (30, 70) # and another line for future telemetry outputs\n",
    "org4 = (30, 90) # and another line for future telemetry outputs\n",
    "org3 = (30, 110) # and another line for future telemetry outputs\n",
    "fontScale = 0.5\n",
    "# white color\n",
    "color = (255, 255, 255)\n",
    "# Line thickness of 2 px\n",
    "thickness = 1\n",
    "\n",
    "# define speed contstants\n",
    "PREFERRED_SPEED = 30 # what it says\n",
    "PREFERRED_SPEED2 = 15 # what it says\n",
    "SPEED_THRESHOLD = 2 #defines when we get close to desired speed so we drop the\n",
    "\n",
    "# Max steering angle\n",
    "MAX_STEER_DEGREES = 50\n",
    "\n",
    "\n",
    "def collision_check(point, obstacles):\n",
    "    collision_free = True\n",
    "    for k in range(len(obstacles)):\n",
    "        collision_dist=math.sqrt((point[0]-obstacles[k][0])**2+(point[1]-obstacles[k][1])**2)\n",
    "        if collision_dist<15:\n",
    "            collision_free=False\n",
    "            break\n",
    "\n",
    "    return collision_free\n",
    "def get_short_dist(point,obstacles):\n",
    "    min_dist=float('inf')\n",
    "    for k in range(len(obstacles)):\n",
    "        collision_dist=math.sqrt((point[0]-obstacles[k][0])**2+(point[1]-obstacles[k][1])**2)\n",
    "        if collision_dist<min_dist:\n",
    "            min_dist=collision_dist\n",
    "\n",
    "    return min_dist\n",
    "\n",
    "def semantic_lidar_data(point_cloud_data,car,l):\n",
    "    l.clear()\n",
    "    c=math.cos(car.get_transform().rotation.yaw*math.pi/180)\n",
    "    s=math.sin(car.get_transform().rotation.yaw*math.pi/180)\n",
    "    for detection in point_cloud_data:\n",
    "        #l.append([round(detection.point.x+car.get_location().x,1),round(detection.point.y+car.get_location().y,1)])\n",
    "        #l.append([detection.point.x+car.get_location().x,detection.point.y+car.get_location().y])\n",
    "        l.append([round((detection.point.x *c +detection.point.y*-s)+car.get_location().x,1),round((detection.point.x*s+detection.point.y*c)+car.get_location().y,1)])\n",
    "\n",
    "def generate_lidar_blueprint(blueprint_library):\n",
    "    lidar_blueprint = blueprint_library.find('sensor.lidar.ray_cast')\n",
    "    lidar_blueprint.set_attribute('channels', str(32))\n",
    "    lidar_blueprint.set_attribute(\"points_per_second\",str(56000))\n",
    "    lidar_blueprint.set_attribute(\"rotation_frequency\",str(1000))\n",
    "    lidar_blueprint.set_attribute(\"range\",str(50))\n",
    "    lidar_blueprint.set_attribute(\"upper_fov\",str(1))\n",
    "    lidar_blueprint.set_attribute(\"lower_fov\",str(-1))\n",
    "#    lidar_blueprint.set_attribute(\"horizontal_fov\",str(360))\n",
    "    return lidar_blueprint\n",
    "\n",
    "\n",
    "# maintain speed function\n",
    "def maintain_speed(s):\n",
    "    ''' \n",
    "    this is a very simple function to maintan desired speed\n",
    "    s arg is actual current speed\n",
    "    '''\n",
    "    if s >= PREFERRED_SPEED:\n",
    "        return 0\n",
    "    elif s < PREFERRED_SPEED - SPEED_THRESHOLD:\n",
    "        return 0.9 # think of it as % of \"full gas\"\n",
    "    else:\n",
    "        return 0.4 # tweak this if the car is way over or under preferred speed \n",
    "def maintain_speed2(s):\n",
    "    ''' \n",
    "    this is a very simple function to maintan desired speed\n",
    "    s arg is actual current speed\n",
    "    '''\n",
    "    if s >= PREFERRED_SPEED2:\n",
    "        return 0\n",
    "    elif s < PREFERRED_SPEED2 - SPEED_THRESHOLD:\n",
    "        return 0.9 # think of it as % of \"full gas\"\n",
    "    else:\n",
    "        return 0.4 # tweak this if the car is way over or under preferred speed \n",
    "\n",
    "wa=world.get_map().get_waypoint(carla.Location(x=-60,y=137,z=0.5))\n",
    "start_point=wa.transform\n",
    "start_point.location.z=0.5\n",
    "midle_point=wa.transform\n",
    "midle_point.location.z=0.5\n",
    "midle_point.location.x=-50\n",
    "\n",
    "vehicle_bp = world.get_blueprint_library().filter('*model3*')\n",
    "vehicle = world.try_spawn_actor(vehicle_bp[0], start_point)\n",
    "vehicle2 = world.try_spawn_actor(vehicle_bp[0], midle_point)\n",
    "\n",
    "\n",
    "camera_bp = world.get_blueprint_library().find('sensor.camera.depth')\n",
    "camera_bp.set_attribute('image_size_x', '512') # this ratio works in CARLA 9.14 on Windows\n",
    "camera_bp.set_attribute('image_size_y', '424')\n",
    "camera_bp.set_attribute('fov', '70')\n",
    "\n",
    "camera_init_trans = carla.Transform(carla.Location(z=CAMERA_POS_Z,x=CAMERA_POS_X))\n",
    "#this creates the camera in the sim\n",
    "camera = world.spawn_actor(camera_bp,camera_init_trans,attach_to=vehicle)\n",
    "\n",
    "def camera_callback(image,data_dict):\n",
    "    data_dict['image'] = np.reshape(np.copy(image.raw_data),(image.height,image.width,4))\n",
    "\n",
    "image_w = camera_bp.get_attribute('image_size_x').as_int()\n",
    "image_h = camera_bp.get_attribute('image_size_y').as_int()\n",
    "\n",
    "camera_data = {'image': np.zeros((image_h,image_w,4))}\n",
    "# this actually opens a live stream from the camera\n",
    "camera.listen(lambda image: camera_callback(image,camera_data))\n",
    "\n",
    "cv2.namedWindow('depth Camera',cv2.WINDOW_AUTOSIZE)\n",
    "cv2.imshow('depth Camera',camera_data['image'])\n",
    "POINTS = o3d.geometry.PointCloud()\n",
    "vis = o3d.visualization.Visualizer()\n",
    "vis.create_window(\n",
    "    window_name='Carla Lidar',\n",
    "    width=960,\n",
    "    height=540,\n",
    "    left=480,\n",
    "    top=270)\n",
    "vis.get_render_option().background_color = [0.05, 0.05, 0.05]\n",
    "vis.get_render_option().point_size = 1\n",
    "vis.get_render_option().show_coordinate_frame = True\n",
    "add_open3d_axis(vis)\n",
    "\n",
    "# Update geometry and camera in game loop\n",
    "frame = 0\n",
    "while True:\n",
    "    \n",
    "    world.tick()\n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        quit = True\n",
    "        vehicle.apply_control(carla.VehicleControl(throttle=0,steer=0,brake=1))\n",
    "        break\n",
    "    image = camera_data['image']\n",
    "    new_image=depth_to_array(image)\n",
    "    POINTS.points=o3d.utility.Vector3dVector(depth_to_local_point_cloud(image,vehicle,max_depth=0.9))\n",
    "\n",
    "    new_image2=new_image.copy()\n",
    "    for x in range(len(new_image2)):\n",
    "        for y in range(len(new_image2[x])):\n",
    "            if new_image2[x][y]<0.2 and x<280:\n",
    "                new_image2[x][y]=16\n",
    "\n",
    "    min_val=float('inf')\n",
    "    for x in range(len(new_image)):\n",
    "        for y in range(len(new_image[x])):\n",
    "            if new_image[x][y]<min_val and x<280:\n",
    "                min_val=new_image[x][y]\n",
    "    if frame == 2:\n",
    "        vis.add_geometry(POINTS)\n",
    "    vis.update_geometry(POINTS)\n",
    "    \n",
    "    vis.poll_events()\n",
    "    vis.update_renderer()\n",
    "    # # This can fix Open3D jittering issues:\n",
    "    time.sleep(0.005)\n",
    "    frame += 1\n",
    "\n",
    "    #image = cv2.putText(image, 'Speed: '+str(int(speed)), org2, font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "    cv2.imshow('depth Camera',new_image2)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "camera.stop()\n",
    "vis.destroy_window()\n",
    "\n",
    "for sensor in world.get_actors().filter('*sensor*'):\n",
    "    sensor.destroy()\n",
    "for actor in world.get_actors().filter('*vehicle*'):\n",
    "    actor.destroy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2a3688d-64f3-487b-a0c2-6dd19687a2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.17.0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o3d.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa96cea9-1521-4481-bd9c-5a46dbb68f55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a5575-253b-41c2-a6cd-b2518989f996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2fcdd8-971c-4b38-8004-fd6183dd92f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9832f759-bd86-4717-88a9-a5e160dbab45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cf41aa-ebdb-452d-9bd1-fee396d7a50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6a6727-05e0-4370-8055-a6c171f775b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c0421-33de-445e-b76a-a98a4b61e3ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
